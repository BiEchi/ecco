# GPT2
gpt2:
    embedding: "transformer.wte.weight" # String. Directly accessed
    type: 'causal'
    activations: # Regex pattern
        - 'mlp\.c_proj'
gpt2-medium:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
        - 'mlp\.c_proj'
gpt2-xl:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
        - 'mlp\.c_proj'
distilgpt2:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
        - 'mlp\.c_proj'
# BERT
distilbert-base-uncased:
    embedding: "embeddings.word_embeddings"
    type: 'mlm'
    activations:
        - 'ffn\.lin2'
bert-base-uncased:
    embedding: "embeddings.word_embeddings"
    type: 'mlm'
    activations:
        - '\d+\.output\.dense' #To set apart from attention.output.dense
bert-large-uncased:
      embedding: "embeddings.word_embeddings"
      type: 'mlm'
      activations:
         - '\d+\.output\.dense'
'distilbert-base-uncased-finetuned-sst-2-english':
    embedding: "embeddings.word_embeddings"
    type: 'mlm'
    activations:
        - 'ffn.lin2'
'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract':
    embedding: "embeddings.word_embeddings"
    type: 'mlm'
    activations:
        - '\d+\.output\.dense'
# ALBERT
albert-base-v2:
      embedding: "embeddings.word_embeddings"
      type: 'mlm'
      activations:
        - '\.ffn_output'
albert-large-v2:
      embedding: "embeddings.word_embeddings"
      type: 'mlm'
      activations:
        - '\.ffn_output'
albert-xxlarge-v2:
      embedding: "embeddings.word_embeddings"
      type: 'mlm'
      activations:
        - '\.ffn_output'
# ROBERTA
roberta-base:
    embedding: 'embeddings.word_embeddings'
    type: 'mlm'
    activations:
        - '\d+\.output\.dense'
roberta-large:
    embedding: 'embeddings.word_embeddings'
    type: 'mlm'
    activations:
        - '\d+\.output\.dense'
# ELECTRA
'google/electra-small-discriminator':
    embedding: 'embeddings.word_embeddings'
    type: 'mlm'
    activations:
        - '\d+\.output\.dense'
'google/electra-base-discriminator':
    embedding: 'embeddings.word_embeddings'
    type: 'mlm'
    activations:
        - '\d+\.output\.dense'
'sentence-transformers/distilbert-base-nli-stsb-mean-tokens':
    embedding: "embeddings.word_embeddings"
    type: 'mlm'
    activations:
        - 'ffn.lin2'
# ENCODER DECODER MODELS
# T5
'tscholak/1wnr382e':
    embedding: 'shared.weight'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
'valhalla/t5-small-qg-hl':
    embedding: 'shared.weight'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
'valhalla/t5-small-qa-qg-hl':
    embedding: 'shared.weight'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
'valhalla/t5-base-e2e-qg':
    embedding: 'shared.weight'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
t5-small:
    embedding: 'shared.weight'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
t5-base:
    embedding: 'shared'
    type: 'enc-dec'
    activations:
        - 'wo' #Note that this will be both encoder and decoder layers
# BART
facebook/bart-large-mnli:
    embedding: 'shared'
    type: 'enc-dec'
    activations:
        - 'fc2'

# DUMMY MODELS
'sshleifer/tiny-gpt2':
  embedding: "transformer.wte.weight"
  type: 'causal'
  activations:
    - 'mlp.c_proj'
'julien-c/bert-xsmall-dummy':
  embedding: "embeddings.word_embeddings"
  type: 'mlm'
  activations:
    - '\d+\.output\.dense'

#EleutherAI
EleutherAI/gpt-neo-125M:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
    - 'mlp\.c_proj'
EleutherAI/gpt-neo-1.3B:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
    - 'mlp\.c_proj'
EleutherAI/gpt-neo-2.7B:
    embedding: "transformer.wte.weight"
    type: 'causal'
    activations:
    - 'mlp\.c_proj'
